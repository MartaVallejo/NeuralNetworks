%{
---------------------------------------------------------------------
Test experiment using a neural network to find an expected answer with a given
function.
Author: Carlos Chavez
---------------------------------------------------------------------
%}
clc;
addpath('C:\code\HW\bbob.v15.03\matlab');
useGeneticAlgorithm = 1;
numInputsNeurons = 2; % use dimensions
numOutputNeurons = 1;
learningRate = 0.1; % Learning rate for backpropagation. Values from 0 to 1
momentum = 0.5; % Delta weight multiplier for backpropagation. Values from 0 to 1
expecteValue = 0.5; % test value
numOfNeuralNetworks = rand([5, 20]); % Random number of neural networks to evaluate
numInitialPopulationGA = rand([5, 20]); % Random number of neural networks to evaluate
numOfIterations = 500; % Random number of iterations to evaluate each neural network

if(useGeneticAlgorithm) % Use Genetic Algorithm
    bestNeuralNetwork;
    % Generate all neural networks to evaluate with different topologies
    for i = 1:numOfNeuralNetworks
        
        numLayers = randi([3, 10]); % Random Number of layers
        topology = zeros(0, numLayers);
        
        % input layer
        topology(numLayers) = numInputsNeurons; % Neurons in input layer
        % for each hidden layer, create random number of neurons
        for j = 2:numLayers - 1
            topology(j) = randi([2, 10]); % Random number of neurons
        end
        topology(numLayers) = numOutputNeurons; % Neurons in last layer
        
        %TEST
        %topology = [2 3 2 4 1];
        
        numInputs = topology(1); % Number of inputs = neurons in first layer
        inputValues = 2*rand(numInputs,1)-1; % Random input values between -1 and 1
        
        % For each topology generate a neural network with same topology and
        % with different random weights (weights are random in the constructor)
        % Generate initial population
        population = NeuralNetwork.empty(numInitialPopulationGA);
        for j = 1:numInitialPopulationGA
            nn = NeuralNetwork(topology, learningRate, momentum);
            population(j) = nn;
        end
        
        isFound = 0;
        bestError = realmax;
        numOfCurrentGenerations = 1;
        while (~isFound && numOfCurrentGenerations < numOfIterations)
            % Evaluate fitness for every element of the population
            
            % Evaluate population
            for j = 1:numInitialPopulationGA
                % Evaluate fitness (fitness is error. The higher the error, the lower the fitness)
                nn = nn.Feedforward(inputValues);
                population(j) = nn;
                
                if(nn.p_error < bestError)
                    prevBestError = bestError;
                    bestError = nn.p_error;
                    bestNeuralNetwork = nn;
                end
                
                if (nn.p_error == 0)
                    isFound = 1;
                    break;
                end
                
            end
            
            % if not found, create new generation
            if (~isFound)
                
                
                % 3) Create new population with new generations
                    for populationIndex = 1:length(population)
                        element1 = population(populationIndex); % element at i (default, should be changed by matingpool)
                        element2 = population(1); % element at first position (default, should be changed by matingpool)
                        randomPos1 = randi([1, length(matingPool)]);
                        randomPos2 = randi([1, length(matingPool)]);
                        
                        if (~isempty(matingPool))
                            % Choose parents using fitness (higher fitness, higher probability) using matingpool
                            element1 = matingPool(randomPos1);
                            element2 = matingPool(randomPos2);
                        end
                        
                        % 3.1) Create new generation with crossover, randomly mutate
                        newGenerationElement = CreateNewGeneration(result, element1, element2);
                        
%                         % For each layer (except output layer)
%                 for numLayer = 1:length(nn.p_layers) - 1
%                     layer = nn.p_layers(numLayer);
%                     % For each neuron
%                     for neuron = 1:length(layer.p_neurons)
%                         
%                     end
%                 end
                        
                        % 3.2) Mutate element
                        newMutatedGenerationElement = MutateElement(result, newGenerationElement);
                        
                        population(populationIndex) = newMutatedGenerationElement;
                    end
                
                
            end
            
            numOfCurrentGenerations = numOfCurrentGenerations + 1;
        end
        
        % TODO: display best weights of neural network with topology = i
        
        
        %nn = nn.TrainNeuralNetwork_GeneticAlgorithm(inputValues, expecteValue, numOfIterations);
    end
else % Use BackPropagation
    % Generate all neural networks to evaluate with different topologies
    for i = 1:numOfNeuralNetworks
        
        numLayers = randi([3, 10]); % Random Number of layers
        topology = zeros(0, numLayers);
        
        % input layer
        topology(numLayers) = numInputsNeurons; % Neurons in input layer
        % for each hidden layer, create random number of neurons
        for j = 2:numLayers - 1
            topology(j) = randi([2, 10]); % Random number of neurons
        end
        topology(numLayers) = numOutputNeurons; % Neurons in last layer
        
        %TEST
        %topology = [2 3 2 4 1];
        
        numInputs = topology(1); % Number of inputs = neurons in first layer
        inputValues = 2*rand(numInputs,1)-1; % Random input values between -1 and 1
        
        % Create neural network with topology
        nn = NeuralNetwork(topology, learningRate, momentum);
        nn = nn.TrainNeuralNetwork_Backpropagation(inputValues, expecteValue, numOfIterations);
        
    end
end

